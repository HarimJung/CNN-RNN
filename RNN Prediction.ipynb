{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 11 characters, 8 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has %d characters, %d unique.' %(data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 0, 'h': 1, 'd': 2, ' ': 3, 'o': 4, 'l': 5, 'e': 6, 'w': 7}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'r', 1: 'h', 2: 'd', 3: ' ', 4: 'o', 5: 'l', 6: 'e', 7: 'w'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 4\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss=0\n",
    "    #forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += (-np.log(ps[t][targets[t], 0]))\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " dehloede hrh hlrhwl ol hwe  rwwrlorloeewlorehedlrdeowr dh lddorwer owewwreoeew  hwd l ew hhwdrl dlhewwlelohelr  rwr rlhoherrl hdl doddolwheldwel olerrod do dd rreehewhldrdworhh orwhlhederrwd eleowl eh \n",
      "----\n",
      "iter 0, loss: 8.317768\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "i=0\n",
    "while i<10:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p.256(tensorflowë¡œ rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 17:01:21.404147 15180 deprecation.py:323] From <ipython-input-3-30fd55ae537c>:9: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0723 17:01:21.412126 15180 deprecation.py:323] From <ipython-input-3-30fd55ae537c>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0723 17:01:21.450065 15180 deprecation.py:506] From C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0723 17:01:21.457031 15180 deprecation.py:506] From C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[-0.77403975]\n",
      " [ 0.2562201 ]\n",
      " [ 0.21532774]\n",
      " [-0.7230842 ]\n",
      " [-0.99690676]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0.]\n",
      "------------------------------\n",
      "init state:\n",
      " [[0.]] \n",
      "X_data val:\n",
      " [[[0. 1. 2. 3.]]] \n",
      "output val:\n",
      " [[[-0.9019123]]] \n",
      "state val:\n",
      " [[-0.9019123]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[[0., 1., 2., 3.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 1\n",
    "X = tf.placeholder(tf.float32, [None, 1, 4])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                              initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, '\\n',v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs,state], feed_dict={X:X_data})\n",
    "    print('init state:\\n', sess.run(initial_state),\n",
    "         '\\nX_data val:\\n', X_data,\n",
    "         '\\noutput val:\\n', outputs_val,\n",
    "         '\\nstate val:\\n',state_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']\n",
      "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']\n",
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[-4.5961630e-01 -3.8549718e-01  2.5870693e-01]\n",
      " [-7.5039923e-02  9.0887189e-02 -5.3411722e-04]\n",
      " [-4.9265543e-01 -7.1770543e-01 -2.6016712e-01]\n",
      " [-3.5555184e-02  4.7998822e-01 -1.9110227e-01]\n",
      " [ 6.5887523e-01 -8.1703484e-02  2.4831617e-01]\n",
      " [ 3.7258518e-01  5.8739328e-01 -7.0440990e-01]\n",
      " [ 6.8356395e-01 -3.8215816e-02 -7.3230761e-01]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0. 0. 0.]\n",
      "------------------------------\n",
      "init state:\n",
      " [[0. 0. 0.]] \n",
      "X_data val:\n",
      " [[[0. 1. 2. 3.]]] \n",
      "output val:\n",
      " [[[-0.8233133   0.09515224 -0.798397  ]]] \n",
      "state val:\n",
      " [[-0.8233133   0.09515224 -0.798397  ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[[0., 1., 2., 3.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 3\n",
    "X = tf.placeholder(tf.float32, [None, 1, 4])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                              initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    print(variables_names)\n",
    "    values = sess.run(variables_names)\n",
    "    print(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, '\\n',v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs,state], feed_dict={X:X_data})\n",
    "    print('init state:\\n', sess.run(initial_state),\n",
    "         '\\nX_data val:\\n', X_data,\n",
    "         '\\noutput val:\\n', outputs_val,\n",
    "         '\\nstate val:\\n',state_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def seqtostr(input):\n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "    \n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n    shared_name=shared_name, name=name)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-797de1911bce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3499\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[1;32m-> 3501\u001b[1;33m                                     return_same_structure)\n\u001b[0m\u001b[0;32m   3502\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3503\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   3010\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3011\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 3012\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   3013\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3014\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2935\u001b[0m         expand_composites=True)\n\u001b[0;32m   2936\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3454\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3455\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3456\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    882\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    883\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;31m# Keras cells always wrap state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_keras_rnn_cell\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     return base_layer.Layer.__call__(\n\u001b[1;32m--> 385\u001b[1;33m         self, inputs, state, scope=scope, *args, **kwargs)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[1;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m       \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1882\u001b[0m     \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m     \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    453\u001b[0m     self._kernel = self.add_variable(\n\u001b[0;32m    454\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         shape=[input_depth + self._num_units, self._num_units])\n\u001b[0m\u001b[0;32m    456\u001b[0m     self._bias = self.add_variable(\n\u001b[0;32m    457\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1482\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;34m\"\"\"Alias for `add_weight`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[1;32m--> 864\u001b[1;33m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n    shared_name=shared_name, name=name)\n"
     ]
    }
   ],
   "source": [
    "data='hihello'\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "X_data = [char_to_ix[ch] for ch in data]\n",
    "X_onehot = tf.one_hot(X_data, vocab_size)\n",
    "\n",
    "batch_size = 1\n",
    "hidden_size = 10\n",
    "seq_length = len(data) -1\n",
    "\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size]), name='weight_hy')\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 1, vocab_size])\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_test = sess.run(X_onehot[0]).reshape(1, vocab_size)\n",
    "    predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "    print(data[0] + seqtostr(predtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a midnight dreary, while I pondered, weak and weary,\n",
      "\n",
      "Over many a quaint and curious volume of forgotten lore-\n",
      "\n",
      "While I nodded, nearly napping, suddenly there came a tapping,\n",
      "\n",
      "As of someone gently rapping, rapping at my chamber door.\n",
      "\n",
      "\"'Tis some visitor,\" I muttered, \"tapping at my chamber door-\n",
      "\n",
      "Only this and nothing more.\"\n",
      "{'e': 0, 'T': 1, \"'\": 2, 'm': 3, 'b': 4, 'I': 5, 'd': 6, 'g': 7, 'r': 8, 't': 9, '\"': 10, 'l': 11, 'y': 12, 'h': 13, 'W': 14, 'a': 15, 'p': 16, 's': 17, '.': 18, '\\n': 19, 'u': 20, 'c': 21, 'O': 22, 'i': 23, 'f': 24, '-': 25, 'n': 26, 'A': 27, ',': 28, 'w': 29, 'v': 30, ' ': 31, 'k': 32, 'o': 33, 'q': 34}\n",
      "step : 0 cost : 100 \n",
      "pred : Obmrrrrrrrrrrrrrrrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrm \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 3.5155835 \n",
      "pred : Ob  c c c,ym  c c c,ymb                                                                                                                                                                                                                                                                                                                          \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 2.9909596 \n",
      "pred : Oe c,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  c \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.6564536 \n",
      "pred : O cua a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a  \n",
      " --------------------------------------------------\n",
      "step : 400 cost : 2.3808992 \n",
      "pred : One cc,re,r a cum  a a ppi e e co,rene co,r.\"bAr a a pping ne a a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a  \n",
      " --------------------------------------------------\n",
      "step : 500 cost : 2.1345863 \n",
      "pred : Onea a my cume a a my a my a my char me co,r.\".\"b\"b\"r a appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.9022261 \n",
      "pred : Onea  co,r.\" ne n a ma moangrea appp ng anly appping no n a mo re, sua m amy a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my c \n",
      " --------------------------------------------------\n",
      "step : 700 cost : 1.6820054 \n",
      "pred : Onea  a myme a m a mdde,rene a m a ma m amy changh any a ma mhorene a m a more,rene a m a ma myme mome eito  e core eor ereanpping any a morene a m a ma  cuang muang cuany appi e  a  amy a mome eongrene a m a ma m amy chang cuany appi e  a  amy a ma m amy a mome eing no ne  am  a my chamber a ma mhorene a m a ma m any a ma myme mome e \n",
      " --------------------------------------------------\n",
      "step : 800 cost : 1.5006725 \n",
      "pred : Only ny chamben ddeorene nddeeroa .\n",
      "b\"btb a mhor eorere cumene tor eamping anly ne morene nddeeroa .\n",
      "b\"btb m arymapping  amping any nodrea ly ny a myme morene nddeeroang, a my chamber a mddeeroa .\n",
      "b\"btb a mhor eorere cumene tor wua m anly eanpping  amling ne nddeeromree coWr.,hg, ne  omene nddeeroangy chamber a a m a m amy apping any  \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 1.4060365 \n",
      "pred : Only ny chamber anly ne mor-\n",
      "\n",
      "\n",
      "As apping any nodreary cua   ama mhor eorereanpping at mu anly appi e  a mdene mor eorere,r apping any nddeorene nddeeroang, a my chamber at dd cuaing cumorene nddeeroangy chor-\n",
      "\n",
      "Os ampping at my chamber a apping any nedrea ly ny cham\" e core-\n",
      "\n",
      "\n",
      "As core vorerhe com\n",
      ".'hg, nedredeor ,\n",
      "-\n",
      "As an meanly ne mor \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = open('input.txt', 'r').read()\n",
    "print(data)\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(char_to_ix)\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data) -1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(1000):\n",
    "        #Test\n",
    "        if step % 100 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆì„¸ì•ˆì„¸ì„¸\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "data = 'ì•ˆë…•í•˜ì„¸ìš”'\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "X_data = [char_to_ix[ch] for ch in data]\n",
    "X_onehot = tf.one_hot(X_data, vocab_size)\n",
    "\n",
    "hidden_size = 10\n",
    "seq_length = len(data)-1\n",
    "batch_size = 1\n",
    "\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 1, vocab_size])\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_test = sess.run(X_onehot[0]).reshape(1, vocab_size)\n",
    "    predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "    print(data[0] + seqtostr(predtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : ì•ˆí•˜ìš”ì•ˆí•˜ \n",
      " --------------------------------------------------\n",
      "step : 50 cost : 0.91580653 \n",
      "pred : ì•ˆë…•í•˜í•˜ìš” \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 0.43975577 \n",
      "pred : ì•ˆë…•í•˜ì„¸ìš” \n",
      " --------------------------------------------------\n",
      "step : 150 cost : 0.21642818 \n",
      "pred : ì•ˆë…•í•˜ì„¸ìš” \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 0.12542069 \n",
      "pred : ì•ˆë…•í•˜ì„¸ìš” \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = \"ì•ˆë…•í•˜ì„¸ìš”\"\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(201):\n",
    "        #Test\n",
    "        if step % 50 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : í—ˆë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•…ë•… \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 4.0819936 \n",
      "pred : í—ˆë•…ê¸¸                                                                                                                                                                                                                                                    \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 3.282193 \n",
      "pred : í—ˆë•…ìš” ë•…                                                                                                                                                                                                                                                  \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.661497 \n",
      "pred : í—ˆë•…ìš” \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      "\n",
      "    ë•…ê¸¸  ë•…ì´ \n",
      "ì°¾ \n",
      " --------------------------------------------------\n",
      "step : 400 cost : 2.1677284 \n",
      "pred : í—ˆì™€ ë•Œ \n",
      "\n",
      "ë ¥  ì°¾ì•„\n",
      "\n",
      "  ë„  ë•…ì´ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ë•…ì„ ë‹¬ ë•…  ê²ƒ\n",
      "ë‹¤ \n",
      " --------------------------------------------------\n",
      "step : 500 cost : 1.7760694 \n",
      "pred : í—ˆì™€ í•˜í•˜ë„\n",
      "\n",
      "ì°¾ ì—\n",
      "ë‹¤ê°€ë‹¤ \n",
      "ë¥¼ ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ ë‹¬ë ¤ê°€ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë•…  ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ ë‹¬ë ¤ê°€ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë•…  ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ ë‹¬ë ¤ê°€ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë•…  ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ ë‹¬ë ¤ê°€ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë•…  ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ ë‹¬ë ¤ê°€ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "    ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë•…  ê²ƒì´ë‹¤\n",
      "\n",
      "  ë˜ \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.4693856 \n",
      "pred : í—ˆì™€ í•˜í•˜ë„\n",
      "\n",
      "ì°¾ ì—\n",
      "ë‹¤ê°€ë‹¤ ë¨¼ \n",
      "ì•„\n",
      "ë‹¤ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„\n",
      "\n",
      " ì°¾ì•„\n",
      "\n",
      "  ë•…ì„ ê²ƒì´ë‹¤\n",
      "\n",
      "    ì°¾ì•„ê°€ë˜ ë‹¬ ê²ƒëŠ” \n",
      "ì„ ëŠ” ì°¾ ê°€ë‹¤ì•„\n",
      "ë¥¼ ì°¾ì•„\n",
      "\n",
      "    ì°¾ì•„ê°€ë‹¤ \n",
      "ë‹¤ \n",
      "\n",
      "ë‹¤ \n",
      "\n",
      "ë‹¤ \n",
      "\n",
      "ì´ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "  ë‚˜ ë¹„ë˜ ë‹¬ \n",
      "ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ \n",
      "\n",
      "ë‹¤ \n",
      "\n",
      "ì´ë‹¤ë„ë˜ ë‹¬ë ¤ ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "ë‹¤ \n",
      "ë‹¤ \n",
      "ë‹¤ê°€ë‹¤ \n",
      "ë¥¼ ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„\n",
      "\n",
      "  ë¹„ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„\n",
      "\n",
      "  ë¹„  ì°¾ì•„\n",
      "\n",
      "   ì°¾ì•„ê°€ë˜ ê²ƒ ë‹¬ê³  ëŠ” ì°¾  \n",
      " --------------------------------------------------\n",
      "step : 700 cost : 1.2245133 \n",
      "pred : í—ˆê³µì˜ ë³„ì˜í•˜ë³„ì•„ê°€ë‹¤ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„ê°€ë‹¤ \n",
      "ë‹¤ \n",
      "ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "ë‹¤ \n",
      "ë‹¤ê°€ë‹¤ \n",
      "   ë‚˜ì„œ\n",
      "ëˆˆë•Œë“¤\n",
      "ì•„ê°€  ì°¾ì•„\n",
      "\n",
      "  ë„ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "  êµ¬ëŠ”ë‹¤\n",
      "ê°€ ë‹¬ë ¤ì˜¤ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "  êµ¬  ë°•ê°€ì— êµ¬\n",
      "ë‹¤ê°€ë‹¤ ê²ƒì´ë‹¤\n",
      "\n",
      " ë‹¬ë©° ë‚ ì•„\n",
      "ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "ì•„\n",
      "ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "  ì„  ì°¾ê°€ë‹¤\n",
      "\n",
      "\n",
      " ë‹¬ë ¤\n",
      "ë‹¤ ê²ƒì´ë‹¤\n",
      "\n",
      "  ë¥¼ ì°¾ì•„ê°€ë‹¤ \n",
      "\n",
      "ì•„\n",
      "ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "  ì„  ì°¾ê°€ì— \n",
      "ì•„\n",
      "ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ \n",
      "\n",
      "ë‹¤ë‹¤\n",
      "\n",
      "\n",
      "ë‹¤\n",
      "\n",
      "\n",
      "\n",
      "ë‹¤ê°€ë‹¤ ë¨¼ ë‹¬ë ¤ì˜¤ë˜ ê²ƒ\n",
      "ë‹¤\n",
      "\n",
      "  êµ¬ëŠ” ê²ƒì„ ê²ƒì´ë‹¤\n",
      "\n",
      "ë‹¤ \n",
      "ì•„ \n",
      " --------------------------------------------------\n",
      "step : 800 cost : 1.0270232 \n",
      "pred : í—ˆê³µì˜ ë³„ì˜í•˜ë³„ë³„ì„ ë‹¬ë ¤ê°€ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "ì•„\n",
      "ë‹¤ \n",
      "\n",
      "ê°€ë‹¤\n",
      "\n",
      "\n",
      "  ë‚˜ ë‚ ì•„ê°€ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "ì•„\n",
      "ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "  ë‚˜ ë‚ ì•„ê°€ë˜ \n",
      " ì˜ ì°¾ì•„ê°€ë‹¤ ì„  ë³„ì˜ ë³„ëˆˆ ë“¤\n",
      "ê°œì¸ ê²ƒì´ë‹¤\n",
      "\n",
      " ë¡œì˜ ë³„í•´ \n",
      "\n",
      "\n",
      "ëŠ” í•œ ë¹„ë³„í–¥í•´ \n",
      "ê°€   êµ¬ë¥´ëŠ” ê¹¡í†µë“¤\n",
      "ê±°ë¦¬ì—  ë‚˜ ì‚¬ëž‘ì„ ë‹¬ë ¤ê°€ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "ì•„\n",
      "ë‹¤ \n",
      "ë‹¤ê°€ë‹¤ ì¸ê°„ì²˜ì—  ë‚˜ê°€ì— ê°€ë‹¤ê°€ë‹¤ ê²ƒì´ë‹¤\n",
      "\n",
      " ë¡œì„ ë‹¬ì§€êµ¬ëŠ” ì„œë ¤ì˜¤ë˜ \n",
      "\n",
      "  ë‚˜ ê·¸ë¦¬ì— ì°¾ì•„ê°€ë‹¤ \n",
      "\n",
      "\n",
      "ë‹¤ê°€ë‹¤ì•„\n",
      "\n",
      "  ë¥¼ ì°¾ì•„ê°€ë‹¤ \n",
      " í•œ ëŠ” ì°¾ ë“¤\n",
      "ì°¾ê°€ ë‹¬ë ¤ê°€ëŠ” í•œ ëŠ” \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 0.8752297 \n",
      "pred : í—ˆê³µì˜ ë³„ì˜í•˜ì„œê°€\n",
      " ê¸¸  ê¸¸ í•˜ë‚˜ê°€\n",
      "\n",
      "ë¡œ ëŠ” ì°¾ë•Œê°€\n",
      "ë¨¼ \n",
      "\n",
      "ê°€ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "    ë³„í•´ ê·¸ê°€ ë‹¬ë ¤ì˜¤ë˜ ê²ƒ\n",
      "\n",
      "\n",
      "\n",
      "  êµ¬ë¥´ëŠ” ê¹¡í†µë“¤\n",
      "ê±°ë¦¬ì— ì €ë¦¬\n",
      "ë¨¼ë¦¬ì— ê°€ë‹¤ ê²ƒì´ë‹¤\n",
      "\n",
      "\n",
      "ê°€ ë‹¬ë ¤\n",
      "ëŠ” í•œ ë¹„ë˜ ë¡œì˜ ê²ƒ\n",
      "ë‹¤ê°€ë˜ ê²ƒì´ë‹¤\n",
      "\n",
      "ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "ë‹¤ \n",
      "ë‹¤ê°€ë‹¤ ì¸ê°„ì²˜ì—  ë‚˜ê°€ë¹„ ê²ƒ ë¹„\n",
      "ë„ˆëŠ” ê²ƒì„ \n",
      "ë¡œì˜ ë‹¬ë ¤ê°€ëŠ” ë‹¬ë©° ë‚ ì•„ê°€ëŠ” í•œ ëŠ” ì°¾ë•Œë“¤\n",
      "ê±°ë¦¬ì— ì°¾ì•„\n",
      "\n",
      "  ë¥¼ ì°¾ì•„ê°€ë‹¤ \n",
      " í•œ ëŠ” ì°¾ë•Œë“¤\n",
      "ê±°ë¦¬ì— ì°¾ì•„\n",
      "\n",
      "  ë‚˜ ë¹„ë˜   êµ¬ë¥´ëŠ” \n",
      "ì§€êµ¬ë¥¼ êµ¬ë¥¼ ì°¾ì•„ê°€ë‹¤ê°€ë‹¤\n",
      "\n",
      "\n",
      "ë˜ ë¨¼ ê²ƒ\n",
      " \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = open('poem.txt', 'r').read()\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(1000):\n",
    "        #Test\n",
    "        if step % 100 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : ì €ë˜ ë˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™Ië˜ë˜ê°™ \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.7694285 \n",
      "pred : ì €  ë‚  fly \n",
      "ë˜ ê°™l  o \n",
      " a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.699877 \n",
      "pred : ì €  ë¡­ê²Œ  ë²—ì–´ ë‚˜ \n",
      "I fly aly aly yo  aê²Œ  ë‚˜ fly aly aly  e a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 1.2253411 \n",
      "pred : ì € ì•„ì¤˜\n",
      "ë˜tìš”ìœ ê°€ì§ˆ ì´ ë– ë‚  ê±°ì´ \n",
      "dor a ayy \n",
      "yy  ì•„ì„ ì…”\n",
      "\n",
      "Take me yo ah\n",
      "  ê¿ˆê°™ì€ my yo doe aywy ë§ í”ì§€ì³¤ì–´ ë‚˜ ì…”k  ord f a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 1200 cost : 0.9571772 \n",
      "pred : ì € ì•„ì¤˜\n",
      " ë¹ ì ¸ì„œ ë‚˜ al my ae\n",
      "y awa y ê³³ì„ ë²—ì–´ ë‚˜ \n",
      "I ì— ì´ ì•„ë“¤\n",
      "ì†Œë¦¬ë˜ ë•Œìœ ê°€ì§ˆ ì´\n",
      "fly ì•„ë‚  ë¡­ê²Œ  ë²—ì–´ ë‚˜ \n",
      "I a awa wyy ë‚˜ aywy ë§ ëŠ˜y fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me yo dord a a a ayy aly aly yo yo yord a a a ayy aly aly yo yo  aay aye y awayy ìˆ¨e ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my yordyìš” \n",
      "le yo  ewyo ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to \n",
      "ê¿ˆIl my aywy ë§ ëŠ˜e yo  ê¿ˆê°™ì€ my yo doe aywy ë§ì•„Take Ta\n",
      "a a ayy y a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 1500 cost : 0.7472068 \n",
      "pred : ì € ì˜¤ëŠ˜ me yoryy \n",
      "ë‚¬ë˜ ê¿ˆê°™ì€ my yordorle any  yo ì•„n\n",
      "fly fwyo ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha any y eie ayy yordyo doe aake me yord a a a awawyw ê³³ì–´ ë‚˜ aly yo yo yord a a a awawy ì–´ ê³³ì–´ ë‚˜ ë– ë‚ ì•„ë“¤í•œ I ì´ y ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shininght light ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my yordd a a a awawayy youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha any y eie ayy yordyo doe aake me to te  lo doe aaway aye\n",
      "yonne  dor a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly awayy aywy ì•„ aì„ ë‚˜ì„ ì…”\n",
      "\n",
      "Tae\n",
      " ê¿ˆê°™ì€ my yo dory ì–´ ê³³ yo ë– ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my yo don akk doelyy aye\n",
      " me  ê¿ˆê°™ì€ my yo yo yo donyyw a awaway-\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha any y eie ayy yordyo doe aake me to te  lo doe aaway aye\n",
      "yonne  dor a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly awayy aywy ì•„ aì„ ë‚˜ì„ ì…”\n",
      "\n",
      "Tae\n",
      " ê¿ˆê°™ì€ my yo dory ì–´ ê³³ yo ë– ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my yo don akk doelyy aye\n",
      " me  ê¿ˆê°™ì€ my yo yo yo donyyw a awaway-\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha any y eie ayy yordyo doe aake me to \n",
      " --------------------------------------------------\n",
      "step : 1800 cost : 0.5968483 \n",
      "pred : ì € ì˜¤ëŠ˜ I mw ake anwy  ë– a ê³³ ì•„ë“¤ê³  Iinghtighnnì €  ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my yo don ao yoryy \n",
      "ley awawaywyy y awawywyë‚˜ \n",
      "lke me  ê¿ˆê°™ì€ mwaywyy aly youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy yordyw\n",
      "waawywyo doe me to Lordyìš” \n",
      "lkuti\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° aha anywyy \n",
      " --------------------------------------------------\n",
      "step : 2100 cost : 0.4761574 \n",
      "pred : ì € ì˜¤ëŠ˜ ë– ë‚˜ìš” ì–´or\n",
      "I Likry fie f a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw ê³³ì–´ ë‚˜ aly youth\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly aywyy ì•„a ë‚  ë²—ì–´ ë‚˜ \n",
      "I  ë– a ê³³ ì•„ë“¤ê³ igin\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane  ì³¤e ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to Lordyìš” ì•„gighnne  ì¹˜ì´ê³  ë˜ ë§ê°€ì§ˆ ë•Œì¯¤\n",
      "ì§€ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw ê³³ì–´ ë‚˜ aly youth\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly aywyy ì•„a ë‚  ë²—ì–´ ë‚˜ \n",
      "I  ë– a ê³³ ì•„ë“¤ê³ igin\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane  ì³¤e ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to Lordyìš” ì•„gighnne  ì¹˜ì´ê³  ë˜ ë§ê°€ì§ˆ ë•Œì¯¤\n",
      "ì§€ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw ê³³ì–´ ë‚˜ aly youth\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly aywyy ì•„a ë‚  ë²—ì–´ ë‚˜ \n",
      "I  ë– a ê³³ ì•„ë“¤ê³ igin\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to te ì•„ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane  ì³¤e ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to Lordyìš” ì•„gighnne  ì¹˜ì´ê³  ë˜ ë§ê°€ì§ˆ ë•Œì¯¤\n",
      "ì§€ì³¤ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw  \n",
      " --------------------------------------------------\n",
      "step : 2400 cost : 0.3947626 \n",
      "pred : ì € ì˜¤ëŠ˜ ë– ë‚˜ìš” ì–´or\n",
      "I Likry fie f a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy aya y awawayy youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly alwayy y awewyo len ae\n",
      "ì‰¬ì§€wì•Šì–´ ë‚˜h\n",
      "Takë¦¬ ê¿ˆì¹˜ì´ê³   oe\n",
      "I anywyw a awawyw bwewyo le\n",
      "yo New Yorky ë‚˜ ë– ë‚ ì•„ë“¤ë©° ë¹›ëŠ˜ry fie a ë– ë‚  ê±°c\n",
      "ì‰¬ì–´ ë‹¤ Io nì¹˜ì´ìš” yo le\n",
      "yonnn ì–´ ë‚˜ always anawywyo doe aaaayy y awawywyë‚˜ ì•„gi \n",
      "ë¡­ê²Œ ì†Œë¦¬ë¥¼ ì§ˆëŸ¬ë„ ì–´ì©” ìˆ˜ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane y ewaaywyway aye y awawawyw awawyw bweway-\n",
      "\n",
      "Take me to London me  ì¹˜ë‚ ê³ ìš” yo le  aawyy ì•„ ë†”ìš” ì œë°œ ì´ ì‰¬ì–´ì©” ìˆ˜ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane y ewaaywyway ayywyouth\n",
      "\n",
      "lkë¦¬ ê¿ˆYoë¦¬ght ì´ c cin më¦¬ í•˜ë©´\n",
      "Shininght ë¹›ì…”\n",
      "Londos ìˆ˜ fly Yor ao ë¹›ëŠ˜lynke me to Lordyìš” ì•„gighnnì €  ì¹˜ì´ê³   ondorin arkì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly aly yo yo yonyorya anawayy alku nìžìœ ë¡­ë‚  ë‚˜ êº¼ ë†”ìš” ì œë°œ ë‚  ì°¾ì§„ ë§ì•„ì¤˜\n",
      "ì‹œë„ëŸ½ê²Œ ì†Œë¦¬ë¥¼ ì§ˆëŸ¬ë„ ì–´ì©” ìˆ˜ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane y ewaaywyway ayywyouth\n",
      "\n",
      "lkë¦¬ ê¿ˆYoë¦¬ght ì´ c c m ì–´ ë‚  ì°¾ì§„ ë§ì•„ì¤˜\n",
      "ì‹œë„ëŸ½ê²Œ ì†Œë¦¬ë¥¼ ì§ˆëŸ¬ë„ ì–´ì©” ìˆ˜ ì—†ì–´ ë‚˜\n",
      "ê°€ë³ê²Œ ì†ì„ í”ë“¤ë©° ane yondonyaky aly youth\n",
      "ìžìœ ë¡­ê²Œ fly ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining light th  \n",
      " --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 2700 cost : 0.3296616 \n",
      "pred : ì € ì˜¤ëŠ˜ ë– ë‚˜ìš” ì–´or\n",
      "I Likry ë‚˜ ìžìœ ë¡­ê²Œ fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayywyouth\n",
      "\n",
      "lì‹œtonle  ê¿ˆê°™ì€ my yo yo yonynk me anawaaanht  ely ë²—ë¦¬ ord ë‚  ë²—ì–´ ë‚˜\n",
      "fl m u ë‚ le yo youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Tnke me to new world aaany bin Pwë¦¬ ë– ë‹¤ë‹ˆëŠ” ìƒˆì²˜ëŸ¼\n",
      "ë‚œ ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayywyouth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Take me to London me  ì¹˜ë‚ ê³ ë‹¤ ì¹˜ì´ë¦¬ryë”” ìžìœ ë¡­ê²Œ fly ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining light light ë¹›ë‚˜ëŠ” my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly always youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any Fly always anawayy y awawayy aly youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ìˆ¨ì–´ ë‚˜ ë¯¸ì³¤ì–´ ë‚˜ ë– ë‚  ê±°ì•¼ ë‹¤ ë¹„ì¼œ\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Take me to London me  ì¹˜ë‚ ê³ ë‹¤ ì¹˜ì´ë¦¬ryë”” ìž \n",
      " --------------------------------------------------\n",
      "step : 3000 cost : 0.42385057 \n",
      "pred : ì € ì˜¤ëŠ˜ ë– ë‚˜ìš” ì–´or\n",
      "I Likë¦¬ì € \n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywhere ì–´ë””ë“ \n",
      "ë‹µë‹µí•œ ì´ ê³³ì„ ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining linht \n",
      "ì†ë‚ ì³¤ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayy ale yo  ê¿ˆì‹œì´ê³ ë¦¬ ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywhere ì–´ë””ë“ \n",
      "ë‹µë‹µí•œ ì´ ê³³ì„ ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining linht \n",
      "ì†ë‚ ì³¤ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayy ale yo  ê¿ˆì‹œì´ê³ ë¦¬ ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywhere ì–´ë””ë“ \n",
      "ë‹µë‹µí•œ ì´ ê³³ì„ ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining linht \n",
      "ì†ë‚ ì³¤ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayy ale yo  ê¿ˆì‹œì´ê³ ë¦¬ ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywhere ì–´ë””ë“ \n",
      "ë‹µë‹µí•œ ì´ ê³³ì„ ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining linht \n",
      "ì†ë‚ ì³¤ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayy ale yo  ê¿ˆì‹œì´ê³ ë¦¬ ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywhere ì–´ë””ë“ \n",
      "ë‹µë‹µí•œ ì´ ê³³ì„ ë²—ì–´ ë‚˜ê¸°ë§Œ í•˜ë©´\n",
      "Shining linht \n",
      "ì†ë‚ ì³¤ì„ ì…”\n",
      "\n",
      "Take me to new world any F a ayy ale yo  ê¿ˆì‹œì´ê³ ë¦¬ ì¹˜ì´ê³  ë¹›ë‚¬ë˜ ê¿ˆê°™ì€ my youth\n",
      "ìžìœ ë¡­ê²Œ fly fly ë‚˜ ìˆ¨ì„ ì…”\n",
      "\n",
      "Take me to new world anywh \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = data = open('song.txt', 'r').read()\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(3001):\n",
    "        #Test\n",
    "        if step % 300 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
