{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 11 characters, 8 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has %d characters, %d unique.' %(data_size, vocab_size))\n",
    "\n",
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 0, 'h': 1, 'd': 2, ' ': 3, 'o': 4, 'l': 5, 'e': 6, 'w': 7}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'r', 1: 'h', 2: 'd', 3: ' ', 4: 'o', 5: 'l', 6: 'e', 7: 'w'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 4\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss=0\n",
    "    #forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += (-np.log(ps[t][targets[t], 0]))\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " dehloede hrh hlrhwl ol hwe  rwwrlorloeewlorehedlrdeowr dh lddorwer owewwreoeew  hwd l ew hhwdrl dlhewwlelohelr  rwr rlhoherrl hdl doddolwheldwel olerrod do dd rreehewhldrdworhh orwhlhederrwd eleowl eh \n",
      "----\n",
      "iter 0, loss: 8.317768\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "i=0\n",
    "while i<10:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p.256(tensorflow로 rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 17:01:21.404147 15180 deprecation.py:323] From <ipython-input-3-30fd55ae537c>:9: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0723 17:01:21.412126 15180 deprecation.py:323] From <ipython-input-3-30fd55ae537c>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0723 17:01:21.450065 15180 deprecation.py:506] From C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0723 17:01:21.457031 15180 deprecation.py:506] From C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[-0.77403975]\n",
      " [ 0.2562201 ]\n",
      " [ 0.21532774]\n",
      " [-0.7230842 ]\n",
      " [-0.99690676]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0.]\n",
      "------------------------------\n",
      "init state:\n",
      " [[0.]] \n",
      "X_data val:\n",
      " [[[0. 1. 2. 3.]]] \n",
      "output val:\n",
      " [[[-0.9019123]]] \n",
      "state val:\n",
      " [[-0.9019123]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[[0., 1., 2., 3.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 1\n",
    "X = tf.placeholder(tf.float32, [None, 1, 4])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                              initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, '\\n',v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs,state], feed_dict={X:X_data})\n",
    "    print('init state:\\n', sess.run(initial_state),\n",
    "         '\\nX_data val:\\n', X_data,\n",
    "         '\\noutput val:\\n', outputs_val,\n",
    "         '\\nstate val:\\n',state_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']\n",
      "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']\n",
      "rnn/basic_rnn_cell/kernel:0 \n",
      " [[-4.5961630e-01 -3.8549718e-01  2.5870693e-01]\n",
      " [-7.5039923e-02  9.0887189e-02 -5.3411722e-04]\n",
      " [-4.9265543e-01 -7.1770543e-01 -2.6016712e-01]\n",
      " [-3.5555184e-02  4.7998822e-01 -1.9110227e-01]\n",
      " [ 6.5887523e-01 -8.1703484e-02  2.4831617e-01]\n",
      " [ 3.7258518e-01  5.8739328e-01 -7.0440990e-01]\n",
      " [ 6.8356395e-01 -3.8215816e-02 -7.3230761e-01]]\n",
      "rnn/basic_rnn_cell/bias:0 \n",
      " [0. 0. 0.]\n",
      "------------------------------\n",
      "init state:\n",
      " [[0. 0. 0.]] \n",
      "X_data val:\n",
      " [[[0. 1. 2. 3.]]] \n",
      "output val:\n",
      " [[[-0.8233133   0.09515224 -0.798397  ]]] \n",
      "state val:\n",
      " [[-0.8233133   0.09515224 -0.798397  ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_data = np.array([[[0., 1., 2., 3.]]])\n",
    "batch_size = 1\n",
    "hidden_size = 3\n",
    "X = tf.placeholder(tf.float32, [None, 1, 4])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                              initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    print(variables_names)\n",
    "    values = sess.run(variables_names)\n",
    "    print(variables_names)\n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k, '\\n',v)\n",
    "    print('-'*30)\n",
    "    outputs_val, state_val = sess.run([outputs,state], feed_dict={X:X_data})\n",
    "    print('init state:\\n', sess.run(initial_state),\n",
    "         '\\nX_data val:\\n', X_data,\n",
    "         '\\noutput val:\\n', outputs_val,\n",
    "         '\\nstate val:\\n',state_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def seqtostr(input):\n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "    \n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n    shared_name=shared_name, name=name)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-797de1911bce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3499\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3500\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[1;32m-> 3501\u001b[1;33m                                     return_same_structure)\n\u001b[0m\u001b[0;32m   3502\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3503\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   3010\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3011\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 3012\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   3013\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3014\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2935\u001b[0m         expand_composites=True)\n\u001b[0;32m   2936\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence_or_composite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3454\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3455\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3456\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    882\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    883\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;31m# Keras cells always wrap state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_keras_rnn_cell\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     return base_layer.Layer.__call__(\n\u001b[1;32m--> 385\u001b[1;33m         self, inputs, state, scope=scope, *args, **kwargs)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m           \u001b[1;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m           \u001b[1;31m# overridden).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m           \u001b[1;31m# Wrapping `call` function in autograph to allow for dynamic control\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1879\u001b[0m       \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1882\u001b[0m     \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m     \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    453\u001b[0m     self._kernel = self.add_variable(\n\u001b[0;32m    454\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         shape=[input_depth + self._num_units, self._num_units])\n\u001b[0m\u001b[0;32m    456\u001b[0m     self._bias = self.add_variable(\n\u001b[0;32m    457\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1482\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;34m\"\"\"Alias for `add_weight`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mgetter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections_arg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    385\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1496\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1237\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    560\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    512\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m\"tensorflow/python\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[1;32m--> 864\u001b[1;33m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    865\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Affinity\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 2023, in variable_v2\n    shared_name=shared_name, name=name)\n"
     ]
    }
   ],
   "source": [
    "data='hihello'\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "X_data = [char_to_ix[ch] for ch in data]\n",
    "X_onehot = tf.one_hot(X_data, vocab_size)\n",
    "\n",
    "batch_size = 1\n",
    "hidden_size = 10\n",
    "seq_length = len(data) -1\n",
    "\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size]), name='weight_hy')\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 1, vocab_size])\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size)\n",
    "initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_test = sess.run(X_onehot[0]).reshape(1, vocab_size)\n",
    "    predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "    print(data[0] + seqtostr(predtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a midnight dreary, while I pondered, weak and weary,\n",
      "\n",
      "Over many a quaint and curious volume of forgotten lore-\n",
      "\n",
      "While I nodded, nearly napping, suddenly there came a tapping,\n",
      "\n",
      "As of someone gently rapping, rapping at my chamber door.\n",
      "\n",
      "\"'Tis some visitor,\" I muttered, \"tapping at my chamber door-\n",
      "\n",
      "Only this and nothing more.\"\n",
      "{'e': 0, 'T': 1, \"'\": 2, 'm': 3, 'b': 4, 'I': 5, 'd': 6, 'g': 7, 'r': 8, 't': 9, '\"': 10, 'l': 11, 'y': 12, 'h': 13, 'W': 14, 'a': 15, 'p': 16, 's': 17, '.': 18, '\\n': 19, 'u': 20, 'c': 21, 'O': 22, 'i': 23, 'f': 24, '-': 25, 'n': 26, 'A': 27, ',': 28, 'w': 29, 'v': 30, ' ': 31, 'k': 32, 'o': 33, 'q': 34}\n",
      "step : 0 cost : 100 \n",
      "pred : Obmrrrrrrrrrrrrrrrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrmmrrm \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 3.5155835 \n",
      "pred : Ob  c c c,ym  c c c,ymb                                                                                                                                                                                                                                                                                                                          \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 2.9909596 \n",
      "pred : Oe c,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  cr cr,ym  cr cr,ym  cr cr,  c \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.6564536 \n",
      "pred : O cua a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a crymr a  \n",
      " --------------------------------------------------\n",
      "step : 400 cost : 2.3808992 \n",
      "pred : One cc,re,r a cum  a a ppi e e co,rene co,r.\"bAr a a pping ne a a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a a a cua a  \n",
      " --------------------------------------------------\n",
      "step : 500 cost : 2.1345863 \n",
      "pred : Onea a my cume a a my a my a my char me co,r.\".\"b\"b\"r a appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co,r.\"b\"bny a mu drea appi e  co \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.9022261 \n",
      "pred : Onea  co,r.\" ne n a ma moangrea appp ng anly appping no n a mo re, sua m amy a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my cham  a my c \n",
      " --------------------------------------------------\n",
      "step : 700 cost : 1.6820054 \n",
      "pred : Onea  a myme a m a mdde,rene a m a ma m amy changh any a ma mhorene a m a more,rene a m a ma myme mome eito  e core eor ereanpping any a morene a m a ma  cuang muang cuany appi e  a  amy a mome eongrene a m a ma m amy chang cuany appi e  a  amy a ma m amy a mome eing no ne  am  a my chamber a ma mhorene a m a ma m any a ma myme mome e \n",
      " --------------------------------------------------\n",
      "step : 800 cost : 1.5006725 \n",
      "pred : Only ny chamben ddeorene nddeeroa .\n",
      "b\"btb a mhor eorere cumene tor eamping anly ne morene nddeeroa .\n",
      "b\"btb m arymapping  amping any nodrea ly ny a myme morene nddeeroang, a my chamber a mddeeroa .\n",
      "b\"btb a mhor eorere cumene tor wua m anly eanpping  amling ne nddeeromree coWr.,hg, ne  omene nddeeroangy chamber a a m a m amy apping any  \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 1.4060365 \n",
      "pred : Only ny chamber anly ne mor-\n",
      "\n",
      "\n",
      "As apping any nodreary cua   ama mhor eorereanpping at mu anly appi e  a mdene mor eorere,r apping any nddeorene nddeeroang, a my chamber at dd cuaing cumorene nddeeroangy chor-\n",
      "\n",
      "Os ampping at my chamber a apping any nedrea ly ny cham\" e core-\n",
      "\n",
      "\n",
      "As core vorerhe com\n",
      ".'hg, nedredeor ,\n",
      "-\n",
      "As an meanly ne mor \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = open('input.txt', 'r').read()\n",
    "print(data)\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(char_to_ix)\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data) -1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(1000):\n",
    "        #Test\n",
    "        if step % 100 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안세안세세\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "data = '안녕하세요'\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "X_data = [char_to_ix[ch] for ch in data]\n",
    "X_onehot = tf.one_hot(X_data, vocab_size)\n",
    "\n",
    "hidden_size = 10\n",
    "seq_length = len(data)-1\n",
    "batch_size = 1\n",
    "\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 1, vocab_size])\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    X_test = sess.run(X_onehot[0]).reshape(1, vocab_size)\n",
    "    predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "    print(data[0] + seqtostr(predtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : 안하요안하 \n",
      " --------------------------------------------------\n",
      "step : 50 cost : 0.91580653 \n",
      "pred : 안녕하하요 \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 0.43975577 \n",
      "pred : 안녕하세요 \n",
      " --------------------------------------------------\n",
      "step : 150 cost : 0.21642818 \n",
      "pred : 안녕하세요 \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 0.12542069 \n",
      "pred : 안녕하세요 \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = \"안녕하세요\"\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(201):\n",
    "        #Test\n",
    "        if step % 50 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : 허땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅땅 \n",
      " --------------------------------------------------\n",
      "step : 100 cost : 4.0819936 \n",
      "pred : 허땅길                                                                                                                                                                                                                                                    \n",
      " --------------------------------------------------\n",
      "step : 200 cost : 3.282193 \n",
      "pred : 허땅요 땅                                                                                                                                                                                                                                                  \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.661497 \n",
      "pred : 허땅요 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      "\n",
      "    땅길  땅이 \n",
      "찾 \n",
      " --------------------------------------------------\n",
      "step : 400 cost : 2.1677284 \n",
      "pred : 허와 때 \n",
      "\n",
      "력  찾아\n",
      "\n",
      "  도  땅이 것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다\n",
      "\n",
      "    땅을 달 땅  것\n",
      "다 \n",
      " --------------------------------------------------\n",
      "step : 500 cost : 1.7760694 \n",
      "pred : 허와 하하도\n",
      "\n",
      "찾 에\n",
      "다가다 \n",
      "를 찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  던 달려가던 것\n",
      "다\n",
      "\n",
      "    찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  땅  것이다\n",
      "\n",
      "  던 달려가던 것\n",
      "다\n",
      "\n",
      "    찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  땅  것이다\n",
      "\n",
      "  던 달려가던 것\n",
      "다\n",
      "\n",
      "    찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  땅  것이다\n",
      "\n",
      "  던 달려가던 것\n",
      "다\n",
      "\n",
      "    찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  땅  것이다\n",
      "\n",
      "  던 달려가던 것\n",
      "다\n",
      "\n",
      "    찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  땅  것이다\n",
      "\n",
      "  던 \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.4693856 \n",
      "pred : 허와 하하도\n",
      "\n",
      "찾 에\n",
      "다가다 먼 \n",
      "아\n",
      "다 것\n",
      "다\n",
      "\n",
      "  를 찾아\n",
      "\n",
      " 찾아\n",
      "\n",
      "  땅을 것이다\n",
      "\n",
      "    찾아가던 달 것는 \n",
      "을 는 찾 가다아\n",
      "를 찾아\n",
      "\n",
      "    찾아가다 \n",
      "다 \n",
      "\n",
      "다 \n",
      "\n",
      "다 \n",
      "\n",
      "이다가다\n",
      "\n",
      "\n",
      "  나 비던 달 \n",
      "다 \n",
      "\n",
      "\n",
      "다 \n",
      "\n",
      "다 \n",
      "\n",
      "이다도던 달려 던 것\n",
      "다\n",
      "\n",
      "다 \n",
      "다 \n",
      "다가다 \n",
      "를 찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  를 찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  를 찾아\n",
      "\n",
      "  비던 것이다\n",
      "\n",
      "  를 찾아\n",
      "\n",
      "  비  찾아\n",
      "\n",
      "   찾아가던 것 달고 는 찾  \n",
      " --------------------------------------------------\n",
      "step : 700 cost : 1.2245133 \n",
      "pred : 허공의 별의하별아가다 것이다\n",
      "\n",
      "  를 찾아가다 \n",
      "다 \n",
      "다가다\n",
      "\n",
      "\n",
      "다 \n",
      "다가다 \n",
      "   나서\n",
      "눈때들\n",
      "아가  찾아\n",
      "\n",
      "  도 것\n",
      "다\n",
      "\n",
      "  구는다\n",
      "가 달려오던 것\n",
      "다\n",
      "\n",
      "  구  박가에 구\n",
      "다가다 것이다\n",
      "\n",
      " 달며 날아\n",
      "던 것이다\n",
      "\n",
      "아\n",
      "다 \n",
      "\n",
      "\n",
      "다가다\n",
      "\n",
      "\n",
      "  을  찾가다\n",
      "\n",
      "\n",
      " 달려\n",
      "다 것이다\n",
      "\n",
      "  를 찾아가다 \n",
      "\n",
      "아\n",
      "다 \n",
      "\n",
      "\n",
      "다가다\n",
      "\n",
      "\n",
      "  을  찾가에 \n",
      "아\n",
      "다 \n",
      "\n",
      "\n",
      "다 \n",
      "\n",
      "다다\n",
      "\n",
      "\n",
      "다\n",
      "\n",
      "\n",
      "\n",
      "다가다 먼 달려오던 것\n",
      "다\n",
      "\n",
      "  구는 것을 것이다\n",
      "\n",
      "다 \n",
      "아 \n",
      " --------------------------------------------------\n",
      "step : 800 cost : 1.0270232 \n",
      "pred : 허공의 별의하별별을 달려가던 것이다\n",
      "\n",
      "아\n",
      "다 \n",
      "\n",
      "가다\n",
      "\n",
      "\n",
      "  나 날아가던 것이다\n",
      "\n",
      "아\n",
      "다 \n",
      "\n",
      "\n",
      "다가다\n",
      "\n",
      "\n",
      "  나 날아가던 \n",
      " 의 찾아가다 을  별의 별눈 들\n",
      "개인 것이다\n",
      "\n",
      " 로의 별해 \n",
      "\n",
      "\n",
      "는 한 비별향해 \n",
      "가   구르는 깡통들\n",
      "거리에  나 사랑을 달려가던 것이다\n",
      "\n",
      "아\n",
      "다 \n",
      "다가다 인간처에  나가에 가다가다 것이다\n",
      "\n",
      " 로을 달지구는 서려오던 \n",
      "\n",
      "  나 그리에 찾아가다 \n",
      "\n",
      "\n",
      "다가다아\n",
      "\n",
      "  를 찾아가다 \n",
      " 한 는 찾 들\n",
      "찾가 달려가는 한 는 \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 0.8752297 \n",
      "pred : 허공의 별의하서가\n",
      " 길  길 하나가\n",
      "\n",
      "로 는 찾때가\n",
      "먼 \n",
      "\n",
      "가던 것이다\n",
      "\n",
      "    별해 그가 달려오던 것\n",
      "\n",
      "\n",
      "\n",
      "  구르는 깡통들\n",
      "거리에 저리\n",
      "먼리에 가다 것이다\n",
      "\n",
      "\n",
      "가 달려\n",
      "는 한 비던 로의 것\n",
      "다가던 것이다\n",
      "\n",
      "다가다\n",
      "\n",
      "\n",
      "다 \n",
      "다가다 인간처에  나가비 것 비\n",
      "너는 것을 \n",
      "로의 달려가는 달며 날아가는 한 는 찾때들\n",
      "거리에 찾아\n",
      "\n",
      "  를 찾아가다 \n",
      " 한 는 찾때들\n",
      "거리에 찾아\n",
      "\n",
      "  나 비던   구르는 \n",
      "지구를 구를 찾아가다가다\n",
      "\n",
      "\n",
      "던 먼 것\n",
      " \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = open('poem.txt', 'r').read()\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(1000):\n",
    "        #Test\n",
    "        if step % 100 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 cost : 100 \n",
      "pred : 저또 또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같I또또같 \n",
      " --------------------------------------------------\n",
      "step : 300 cost : 2.7694285 \n",
      "pred : 저  날 fly \n",
      "또 같l  o \n",
      " a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 600 cost : 1.699877 \n",
      "pred : 저  롭게  벗어 나 \n",
      "I fly aly aly yo  a게  나 fly aly aly  e a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 900 cost : 1.2253411 \n",
      "pred : 저 아줘\n",
      "또t요유가질 이 떠날 거이 \n",
      "dor a ayy \n",
      "yy  아을 셔\n",
      "\n",
      "Take me yo ah\n",
      "  꿈같은 my yo doe aywy 말 흔지쳤어 나 셔k  ord f a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 1200 cost : 0.9571772 \n",
      "pred : 저 아줘\n",
      " 빠져서 나 al my ae\n",
      "y awa y 곳을 벗어 나 \n",
      "I 에 이 아들\n",
      "소리또 때유가질 이\n",
      "fly 아날 롭게  벗어 나 \n",
      "I a awa wyy 나 aywy 말 늘y fly 나 숨을 셔\n",
      "\n",
      "Take me yo dord a a a ayy aly aly yo yo yord a a a ayy aly aly yo yo  aay aye y awayy 숨e 빛났던 꿈같은 my yordy요 \n",
      "le yo  ewyo 자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to \n",
      "꿈Il my aywy 말 늘e yo  꿈같은 my yo doe aywy 말아Take Ta\n",
      "a a ayy y a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a  \n",
      " --------------------------------------------------\n",
      "step : 1500 cost : 0.7472068 \n",
      "pred : 저 오늘 me yoryy \n",
      "났던 꿈같은 my yordorle any  yo 아n\n",
      "fly fwyo 자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha any y eie ayy yordyo doe aake me yord a a a awawyw 곳어 나 aly yo yo yord a a a awawy 어 곳어 나 떠날아들한 I 이 y 벗어 나기만 하면\n",
      "Shininght light 빛났던 꿈같은 my yordd a a a awawayy youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha any y eie ayy yordyo doe aake me to te  lo doe aaway aye\n",
      "yonne  dor a 숨어 나 미쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly awayy aywy 아 a을 나을 셔\n",
      "\n",
      "Tae\n",
      " 꿈같은 my yo dory 어 곳 yo 떠이고 빛났던 꿈같은 my yo don akk doelyy aye\n",
      " me  꿈같은 my yo yo yo donyyw a awaway-\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha any y eie ayy yordyo doe aake me to te  lo doe aaway aye\n",
      "yonne  dor a 숨어 나 미쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly awayy aywy 아 a을 나을 셔\n",
      "\n",
      "Tae\n",
      " 꿈같은 my yo dory 어 곳 yo 떠이고 빛났던 꿈같은 my yo don akk doelyy aye\n",
      " me  꿈같은 my yo yo yo donyyw a awaway-\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha any y eie ayy yordyo doe aake me to \n",
      " --------------------------------------------------\n",
      "step : 1800 cost : 0.5968483 \n",
      "pred : 저 오늘 I mw ake anwy  떠a 곳 아들고 Iinghtighnn저  치이고 빛났던 꿈같은 my yo don ao yoryy \n",
      "ley awawaywyy y awawywy나 \n",
      "lke me  꿈같은 mwaywyy aly youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy yordyw\n",
      "waawywyo doe me to Lordy요 \n",
      "lkuti\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 aha anywyy \n",
      " --------------------------------------------------\n",
      "step : 2100 cost : 0.4761574 \n",
      "pred : 저 오늘 떠나요 어or\n",
      "I Likry fie f a 숨어 나 미쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw 곳어 나 aly youth\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly aywyy 아a 날 벗어 나 \n",
      "I  떠a 곳 아들고igin\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 ane  쳤e 나 숨을 셔\n",
      "\n",
      "Take me to Lordy요 아gighnne  치이고 또 망가질 때쯤\n",
      "지쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw 곳어 나 aly youth\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly aywyy 아a 날 벗어 나 \n",
      "I  떠a 곳 아들고igin\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 ane  쳤e 나 숨을 셔\n",
      "\n",
      "Take me to Lordy요 아gighnne  치이고 또 망가질 때쯤\n",
      "지쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw 곳어 나 aly youth\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly aywyy 아a 날 벗어 나 \n",
      "I  떠a 곳 아들고igin\n",
      "자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to te 아 없어 나\n",
      "가볍게 손을 흔들며 ane  쳤e 나 숨을 셔\n",
      "\n",
      "Take me to Lordy요 아gighnne  치이고 또 망가질 때쯤\n",
      "지쳤어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to London a a awawyw  \n",
      " --------------------------------------------------\n",
      "step : 2400 cost : 0.3947626 \n",
      "pred : 저 오늘 떠나요 어or\n",
      "I Likry fie f a 숨어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy aya y awawayy youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly alwayy y awewyo len ae\n",
      "쉬지w않어 나h\n",
      "Tak리 꿈치이고  oe\n",
      "I anywyw a awawyw bwewyo le\n",
      "yo New Yorky 나 떠날아들며 빛늘ry fie a 떠날 거c\n",
      "쉬어 다 Io n치이요 yo le\n",
      "yonnn 어 나 always anawywyo doe aaaayy y awawywy나 아gi \n",
      "롭게 소리를 질러도 어쩔 수 없어 나\n",
      "가볍게 손을 흔들며 ane y ewaaywyway aye y awawawyw awawyw bweway-\n",
      "\n",
      "Take me to London me  치날고요 yo le  aawyy 아 놔요 제발 이 쉬어쩔 수 없어 나\n",
      "가볍게 손을 흔들며 ane y ewaaywyway ayywyouth\n",
      "\n",
      "lk리 꿈Yo리ght 이 c cin m리 하면\n",
      "Shininght 빛셔\n",
      "Londos 수 fly Yor ao 빛늘lynke me to Lordy요 아gighnn저  치이고  ondorin ark어 나 떠날 거야 다 비켜\n",
      "I fly aly yo yo yonyorya anawayy alku n자유롭날 나 꺼 놔요 제발 날 찾진 말아줘\n",
      "시끄럽게 소리를 질러도 어쩔 수 없어 나\n",
      "가볍게 손을 흔들며 ane y ewaaywyway ayywyouth\n",
      "\n",
      "lk리 꿈Yo리ght 이 c c m 어 날 찾진 말아줘\n",
      "시끄럽게 소리를 질러도 어쩔 수 없어 나\n",
      "가볍게 손을 흔들며 ane yondonyaky aly youth\n",
      "자유롭게 fly 나기만 하면\n",
      "Shining light th  \n",
      " --------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 2700 cost : 0.3296616 \n",
      "pred : 저 오늘 떠나요 어or\n",
      "I Likry 나 자유롭게 fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any F a ayywyouth\n",
      "\n",
      "l시tonle  꿈같은 my yo yo yonynk me anawaaanht  ely 벗리 ord 날 벗어 나\n",
      "fl m u 날le yo youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any F a 숨어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Tnke me to new world aaany bin Pw리 떠다니는 새처럼\n",
      "난 자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any F a ayywyouth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any F a 숨어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Take me to London me  치날고다 치이리ry디 자유롭게 fly 나기만 하면\n",
      "Shining light light 빛나는 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly always youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any Fly always anawayy y awawayy aly youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world any F a 숨어 나 미쳤어 나 떠날 거야 다 비켜\n",
      "I fly away-\n",
      "\n",
      "Take me to new world anayy alway-\n",
      "\n",
      "Take me to London me  치날고다 치이리ry디 자 \n",
      " --------------------------------------------------\n",
      "step : 3000 cost : 0.42385057 \n",
      "pred : 저 오늘 떠나요 어or\n",
      "I Lik리저 \n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywhere 어디든\n",
      "답답한 이 곳을 벗어 나기만 하면\n",
      "Shining linht \n",
      "손날쳤을 셔\n",
      "\n",
      "Take me to new world any F a ayy ale yo  꿈시이고리 치이고 빛났던 꿈같은 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywhere 어디든\n",
      "답답한 이 곳을 벗어 나기만 하면\n",
      "Shining linht \n",
      "손날쳤을 셔\n",
      "\n",
      "Take me to new world any F a ayy ale yo  꿈시이고리 치이고 빛났던 꿈같은 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywhere 어디든\n",
      "답답한 이 곳을 벗어 나기만 하면\n",
      "Shining linht \n",
      "손날쳤을 셔\n",
      "\n",
      "Take me to new world any F a ayy ale yo  꿈시이고리 치이고 빛났던 꿈같은 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywhere 어디든\n",
      "답답한 이 곳을 벗어 나기만 하면\n",
      "Shining linht \n",
      "손날쳤을 셔\n",
      "\n",
      "Take me to new world any F a ayy ale yo  꿈시이고리 치이고 빛났던 꿈같은 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywhere 어디든\n",
      "답답한 이 곳을 벗어 나기만 하면\n",
      "Shining linht \n",
      "손날쳤을 셔\n",
      "\n",
      "Take me to new world any F a ayy ale yo  꿈시이고리 치이고 빛났던 꿈같은 my youth\n",
      "자유롭게 fly fly 나 숨을 셔\n",
      "\n",
      "Take me to new world anywh \n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # To prevent graph error of tensorflow\n",
    "\n",
    "# For convenience\n",
    "def seqtostr(input): \n",
    "    return ''.join(ix_to_char[ch] for ch in input[:])\n",
    "\n",
    "# Kapathy's test function\n",
    "def sample(x, n, cell, Why, by):\n",
    "    W = cell[0]\n",
    "    Wxh = W[:vocab_size]\n",
    "    Whh = W[vocab_size:]\n",
    "    bh = cell[1]\n",
    "    h = np.zeros((1, hidden_size))\n",
    "    ixes = []\n",
    "\n",
    "    for step in range(n):\n",
    "        h = np.tanh(np.dot(x, Wxh) + np.dot(h, Whh) + bh)\n",
    "        y = np.dot(h, Why) + by\n",
    "        # p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # ix = np.random.choice(list(range(vocab_size)), p=p.ravel())\n",
    "        ix = np.argmax(y)\n",
    "        x = np.zeros((1, vocab_size))\n",
    "        x[0][ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "# Karpathy's preprocessing\n",
    "data = data = open('song.txt', 'r').read()\n",
    "\n",
    "chars= list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Preparing training\n",
    "inputs = [[char_to_ix[ch] for ch in data[:-1]]]\n",
    "targets = [[char_to_ix[ch] for ch in data[1:]]]\n",
    "X_train = tf.one_hot(inputs, vocab_size)\n",
    "Y_train = tf.one_hot(targets, vocab_size)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "cost_val = 100 # initial cost\n",
    "learning_rate = 1e-3 #0.001\n",
    "\n",
    "# etc.\n",
    "batch_size = 1\n",
    "seq_length = len(data)-1\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, vocab_size])\n",
    "Y = tf.placeholder(tf.int32, [None, seq_length, vocab_size])\n",
    "\n",
    "# Output layer of char-rnn\n",
    "Why = tf.Variable(tf.random_normal([batch_size, hidden_size, vocab_size], name='weight_hy'))\n",
    "by = tf.Variable(tf.random_normal([batch_size, 1, vocab_size]), name='bias_y')\n",
    "\n",
    "# Setting cell, loss function, optimizer\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=state, dtype=tf.float32)\n",
    "logit = tf.matmul(outputs, Why) + by\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y))\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for step in range(3001):\n",
    "        #Test\n",
    "        if step % 300 == 0:\n",
    "            X_test = sess.run(X_train[0][0]).reshape(1, vocab_size) # Start from the beginning\n",
    "            predtxt = sample(X_test, seq_length, sess.run(cell.variables), sess.run(Why), sess.run(by))\n",
    "            print('step :', step, 'cost :', cost_val,\n",
    "                  '\\npred :', data[0] + seqtostr(predtxt), '\\n','-'*50)\n",
    "            \n",
    "        # Train\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X:sess.run(X_train), Y:sess.run(Y_train)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
